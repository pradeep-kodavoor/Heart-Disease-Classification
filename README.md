# â¤ï¸ Heart Disease Classification â€” ML Assignment 2

## ğŸ“Œ Problem Statement

Heart disease is one of the leading causes of death globally. Early detection and accurate prediction of heart disease can significantly improve patient outcomes and help healthcare professionals make informed decisions. 

The objective of this project is to build and compare multiple machine learning classification models that can predict whether a patient has heart disease based on clinical attributes. We implement 6 different ML algorithms, evaluate them using 6 standard metrics, and deploy an interactive Streamlit web application for real-time model inference and comparison.

---

## ğŸ“Š Dataset Description

| Property | Details |
|----------|---------|
| **Dataset Name** | Heart Disease Dataset |
| **Source** | [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/45/heart+disease) / [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) |
| **Type** | Binary Classification |
| **Number of Instances** | 1025 |
| **Number of Features** | 13 |
| **Target Variable** | `target` (0 = No Heart Disease, 1 = Heart Disease) |
| **Missing Values** | None |

### Feature Descriptions

| # | Feature | Description | Type |
|---|---------|-------------|------|
| 1 | `age` | Age of the patient in years | Numeric |
| 2 | `sex` | Sex (1 = Male, 0 = Female) | Binary |
| 3 | `cp` | Chest pain type (0â€“3) | Categorical |
| 4 | `trestbps` | Resting blood pressure (mm Hg) | Numeric |
| 5 | `chol` | Serum cholesterol (mg/dl) | Numeric |
| 6 | `fbs` | Fasting blood sugar > 120 mg/dl (1 = True, 0 = False) | Binary |
| 7 | `restecg` | Resting ECG results (0â€“2) | Categorical |
| 8 | `thalach` | Maximum heart rate achieved | Numeric |
| 9 | `exang` | Exercise-induced angina (1 = Yes, 0 = No) | Binary |
| 10 | `oldpeak` | ST depression induced by exercise relative to rest | Numeric |
| 11 | `slope` | Slope of the peak exercise ST segment (0â€“2) | Categorical |
| 12 | `ca` | Number of major vessels colored by fluoroscopy (0â€“4) | Numeric |
| 13 | `thal` | Thalassemia (0 = Normal, 1 = Fixed Defect, 2 = Reversible Defect) | Categorical |

### Class Distribution
- **0 (No Heart Disease):** ~499 instances
- **1 (Heart Disease):** ~526 instances
- The dataset is roughly balanced.

---

## ğŸ¤– Models Used

Six classification models were implemented and evaluated:

1. **Logistic Regression** â€” A linear model for binary classification using the sigmoid function
2. **Decision Tree Classifier** â€” A tree-based model that learns decision rules from features
3. **K-Nearest Neighbors (KNN)** â€” A distance-based lazy learner (k=5)
4. **Naive Bayes (Gaussian)** â€” A probabilistic model based on Bayes' theorem
5. **Random Forest (Ensemble)** â€” An ensemble of decision trees using bagging
6. **XGBoost (Ensemble)** â€” A gradient boosting ensemble method

### ğŸ“ˆ Model Comparison Table

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
|---------------|----------|-----|-----------|--------|----|-----|
| Logistic Regression | 0.8098 | 0.9298 | 0.7619 | 0.9143 | 0.8312 | 0.6309 |
| Decision Tree | 0.9854 | 0.9857 | 1.0000 | 0.9714 | 0.9855 | 0.9712 |
| KNN | 0.8634 | 0.9629 | 0.8738 | 0.8571 | 0.8654 | 0.7269 |
| Naive Bayes | 0.8293 | 0.9043 | 0.8070 | 0.8762 | 0.8402 | 0.6602 |
| Random Forest (Ensemble) | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 |
| XGBoost (Ensemble) | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 |

---

## Observations on Model Performance

| ML Model Name | Observation |
|---------------|-------------|
| **Logistic Regression** | At around 81% accuracy, Logistic Regression acts as a reasonable starting point. Its precision is the lowest among all models (0.76), which means it tends to flag some healthy patients as diseased. On the other hand, its recall is quite strong (0.91), so it catches most actual disease cases â€” a useful trait in a medical screening context where missing a sick patient is more costly than a false alarm. The limitation comes from its linear nature; it draws a straight boundary between classes and cannot capture the more complex feature interactions present in clinical data. Still, its AUC of 0.93 shows it ranks patients fairly well overall. |
| **Decision Tree** | The Decision Tree performs impressively at 98.54% accuracy with perfect precision â€” every patient it labels as diseased truly has the condition. It only misses a handful of actual cases (recall = 0.97). This strong result comes from its ability to split on multiple features in a hierarchical fashion, naturally capturing interactions like "older age + high cholesterol + chest pain type 3." That said, the near-perfect numbers are partly inflated by duplicate rows in the dataset, and without pruning, decision trees tend to memorize training data rather than generalize from it. |
| **KNN** | KNN lands in the middle at 86.34% accuracy, with precision and recall fairly balanced around 0.86â€“0.87. Its AUC of 0.96 is actually quite competitive, suggesting it separates the two classes well even if its hard predictions aren't always right. KNN works by looking at a patient's 7 nearest neighbors in the feature space and taking a vote. The challenge with 13 features is that distances become less meaningful in higher dimensions â€” this is the well-known "curse of dimensionality." Proper feature scaling (applied via StandardScaler) helps but doesn't fully overcome this limitation. |
| **Naive Bayes** | Naive Bayes comes in at 82.93%, making it the second-weakest performer. It assumes each feature contributes independently to the prediction â€” an assumption clearly violated here since features like age, blood pressure, cholesterol, and heart rate are medically correlated. This leads to a relatively low MCC of 0.66, indicating its predictions are less balanced across the two classes. However, for such a simple and fast algorithm, the AUC of 0.90 is respectable and shows it still captures the general direction of risk reasonably well. |
| **Random Forest (Ensemble)** | Random Forest hits 100% on every metric. It aggregates predictions from 150 individually trained decision trees, each built on a random data subset with a random feature subset. This diversity among trees reduces variance and makes the ensemble far more robust than any single tree. The perfect score is noteworthy but should be interpreted cautiously â€” the Heart Disease dataset contains duplicate rows, and some test samples are likely near-identical to training ones. With truly unseen patient data, we would expect slightly lower (but still strong) performance. The key takeaway is the clear benefit of bagging-based ensembles over individual learners. |
| **XGBoost (Ensemble)** | XGBoost matches Random Forest with perfect scores across the board. Unlike Random Forest's parallel tree-building approach, XGBoost constructs trees one at a time â€” each new tree specifically targets the mistakes of the previous ones. This sequential error-correction, combined with built-in L1/L2 regularization, makes it one of the most effective algorithms for tabular data. As with Random Forest, the 100% metrics are partly a consequence of dataset duplicates rather than pure generalization ability. In a production clinical setting, XGBoost would likely still be among the top performers but with slightly more realistic accuracy numbers. |

### Summary
- The two ensemble methods â€” Random Forest and XGBoost â€” clearly dominate, highlighting how combining multiple models leads to better predictions than any single algorithm.
- Decision Tree comes close to the ensembles, but standalone trees risk overfitting without careful tuning.
- KNN offers decent middle-ground results, though its effectiveness diminishes as feature count grows.
- Logistic Regression provides a solid baseline with excellent recall, making it suitable for initial screening despite its linear constraints.
- Naive Bayes ranks last, held back by its independence assumption which doesn't hold for correlated clinical measurements.
- It's worth noting that the perfect ensemble scores are partly explained by duplicate records in the dataset. Deduplicating the data before splitting would yield more conservative and realistic performance numbers.

---

## ğŸš€ Streamlit App Features

The deployed Streamlit application includes:

1. **ğŸ“ CSV Upload** â€” Upload test data for evaluation
2. **ğŸ” Model Selection Dropdown** â€” Choose from 6 trained models
3. **ğŸ“ˆ Evaluation Metrics Display** â€” View Accuracy, AUC, Precision, Recall, F1, and MCC
4. **ğŸ”¢ Confusion Matrix** â€” Visual heatmap of prediction results
5. **ğŸ“ Classification Report** â€” Detailed per-class precision, recall, and F1
6. **ğŸ“Š All Models Comparison Table** â€” Side-by-side comparison with best values highlighted
7. **ğŸ”® Predictions Preview** â€” See individual predictions with probabilities

---

## ğŸ› ï¸ How to Run Locally

### Prerequisites
- Python 3.8+
- pip

### Steps

1. **Clone the repository:**
   ```bash
   git clone https://github.com/pradeep-kodavoor/Heart-Disease-Classification.git
   cd Heart-Disease-Classification
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download the dataset:**
   - Download `heart.csv` from [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)
   - Place it in the project root directory

4. **Train the models:**
   ```bash
   python model/model_training.py
   ```
   This will save all trained models as `.pkl` files in the `model/` folder.

5. **Run the Streamlit app:**
   ```bash
   streamlit run app.py
   ```

6. **Test the app:**
   - Upload the `test_data.csv` generated during training
   - Select different models and view results

---

## ğŸ“‚ Project Structure

```
Heart-Disease-Classification/
â”‚â”€â”€ app.py                    # Streamlit web application
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ README.md                 # This file
â”‚â”€â”€ heart.csv                 # Dataset (download from Kaggle)
â”‚â”€â”€ test_data.csv             # Test data for app demo (auto-generated)
â”‚â”€â”€ model/
â”‚   â”œâ”€â”€ model_training.py     # Training script for all 6 models
â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚   â”œâ”€â”€ decision_tree.pkl
â”‚   â”œâ”€â”€ knn.pkl
â”‚   â”œâ”€â”€ naive_bayes.pkl
â”‚   â”œâ”€â”€ random_forest_ensemble.pkl
â”‚   â”œâ”€â”€ xgboost_ensemble.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”œâ”€â”€ feature_names.pkl
â”‚   â”œâ”€â”€ model_results.csv
â”‚   â”œâ”€â”€ confusion_matrices.png
â”‚   â””â”€â”€ model_comparison.png
```

---

## ğŸ”— Links

- **GitHub Repository:** [Heart-Disease-Classification](https://github.com/pradeep-kodavoor/Heart-Disease-Classification)
- **Live Streamlit App:** [Heart-Disease-Classification-App](https://heart-disease-classification-app.streamlit.app)

---

## ğŸ“š References

1. UCI Heart Disease Dataset: https://archive.ics.uci.edu/dataset/45/heart+disease
2. Scikit-learn Documentation: https://scikit-learn.org/stable/
3. XGBoost Documentation: https://xgboost.readthedocs.io/
4. Streamlit Documentation: https://docs.streamlit.io/
